{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n# @hidden_cell\ncredentials_2 = {\n  'password':\"\"\"cef302046fdb4a5a95a34eac2ea36fdf5972b4ceb5d1c10c98cf2589be7218f5\"\"\",\n  'custom_url':'https://f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix:cef302046fdb4a5a95a34eac2ea36fdf5972b4ceb5d1c10c98cf2589be7218f5@f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix.cloudant.com',\n  'username':'f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix',\n  'url':'https://undefined'\n}\n# @hidden_cell\ncredentials_2 = {\n  'password':\"\"\"cef302046fdb4a5a95a34eac2ea36fdf5972b4ceb5d1c10c98cf2589be7218f5\"\"\",\n  'custom_url':'https://f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix:cef302046fdb4a5a95a34eac2ea36fdf5972b4ceb5d1c10c98cf2589be7218f5@f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix.cloudant.com',\n  'username':'f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix',\n  'url':'https://undefined'\n}\n# Visualizing data stored in Cloudant DB from Watson IoT Platform and Anomaly Detection by using IBM Watson Studio", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Install the spark-sql-cloudant package (library for reading data from Cloudant database using Spark SQL) in your IBM Watson Studio account by executing the following cell, and then restart the kernel. You will use **pixiedust** to import the required packages.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "!pip install --upgrade pixiedust\nimport pixiedust\n# Use play-json version 2.5.9. Latest version is not supported at this time.\npixiedust.installPackage(\"com.typesafe.play:play-json_2.11:2.5.9\")\n# Get the matching version sql-cloudant library\npixiedust.installPackage(\"org.apache.bahir:spark-sql-cloudant_2.11:0\")\n# To fix PyJavaError\npixiedust.packageManager.uninstallPackage(\"org.apache.bahir:spark-sql-cloudant_2.11:2.2.1\")", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already up-to-date: pixiedust in /usr/local/src/bluemix_jupyter_bundle.v103/notebook/lib/extra (1.1.15)\nRequirement not upgraded as not directly required: colour in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (0.1.5)\nRequirement not upgraded as not directly required: lxml in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (4.1.0)\nRequirement not upgraded as not directly required: geojson in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (1.3.5)\nRequirement not upgraded as not directly required: astunparse in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (1.5.0)\nRequirement not upgraded as not directly required: requests in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (2.18.4)\nRequirement not upgraded as not directly required: markdown in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (2.6.9)\nRequirement not upgraded as not directly required: mpld3 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust) (0.3)\nRequirement not upgraded as not directly required: wheel<1.0,>=0.23.0 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from astunparse->pixiedust) (0.31.1)\nRequirement not upgraded as not directly required: six<2.0,>=1.6.1 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from astunparse->pixiedust) (1.11.0)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->pixiedust) (3.0.4)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->pixiedust) (2.6)\nRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->pixiedust) (1.22)\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in /usr/local/src/conda3_runtime.v53/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->pixiedust) (2018.11.29)\n\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\nPixiedust database opened successfully\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version 1.1.15</span>\n        </div>\n        "
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Package already installed: com.typesafe.play:play-json_2.11:2.5.9\nDownloading package org.apache.bahir:spark-sql-cloudant_2.11:2.3.2 to /gpfs/fs01/user/sa33-2c209a053f71d8-2331b328e772/data/libs/spark-sql-cloudant_2.11-2.3.2.jar\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "\n                <div>\n                    <span id=\"pm_label1d79fa96\">Starting download...</span>\n                    <progress id=\"pm_progress1d79fa96\" max=\"100\" value=\"0\" style=\"width:200px\"></progress>\n                </div>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 8192 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 5.68);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 16384 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 11.37);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 24576 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 17.05);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 32768 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 22.73);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 40960 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 28.41);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 49152 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 34.1);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 57344 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 39.78);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 65536 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 45.46);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 73728 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 51.15);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 81920 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 56.83);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 90112 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 62.51);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 98304 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 68.19);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 106496 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 73.88);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 114688 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 79.56);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 122880 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 85.24);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 131072 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 90.92);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 139264 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 96.61);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n                    $(\"#pm_label1d79fa96\").text(\"Downloaded 144154 of 144154 bytes\");\n                    $(\"#pm_progress1d79fa96\").attr(\"value\", 100.0);\n                ", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Package org.apache.bahir:spark-sql-cloudant_2.11:2.3.2 downloaded successfully\n\u001b[31mPlease restart Kernel to complete installation of the new package\u001b[0m\nSuccessfully added package org.apache.bahir:spark-sql-cloudant_2.11:2.3.2\nSuccessfully deleted package org.apache.bahir:spark-sql-cloudant_2.11:2.3.2\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "**User input required** : Cloudant credentials.\n\nIf you have a connection with Cloudant set up for this project, complete the following steps:\n1.\tImport your Cloudant credentials by clicking on the following cell below to select it\n2.\tClick **Find and Add Data** \n3.\tSelect the Connections tab and click on **Insert to code**. A dictionary called *credentials_1* is  added to the cell that contains the Cloudant credentials. If the dictionary has another name, change it to *credentials_1*. \n4.\tRun the cell. \n\nIf you don\u2019t have a connection with Cloudant set up, the credentials can be found on IBM Cloud dashboard by completing the following steps:\n\n1.\tGo to your Cloudant service on IBM Cloud, \n2.\tSelect its Service Credentials section on the left \n3.\tClick **View Credentials** to view the username and password. \n4.\tUpdate the *username* and *password* variables with Cloudant\u2019s username and password.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# This empty cell will be populated with your Cloudant credentials if you follow the steps explained above.", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "username = \"f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix\"\npassword = \"cef302046fdb4a5a95a34eac2ea36fdf5972b4ceb5d1c10c98cf2589be7218f5\"", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "host = 'f21c85c3-b09f-48bf-80cf-2c0b33ac3fa7-bluemix' + '.cloudant.com'", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "**User input required**: Cloudant database name.\n\nIf you are not sure which database contains the data that you want to import, go to your Cloudant service on IBM Cloud and click **Launch** to display the database name. Update the *dbName* variable with the database name, for example\n *iotp_abcdef_default_2018-01-10*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "dbName = \"iotp_23vyzz_devicedata_2019-03\"", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "Connect to the Cloudant database that is generated by WIoTP connector for historical data.\n\nThe following code connects to Cloudant NoSQL DB and returns an RDD data frame for the stored IoT data. The line `option(\"jsonstore.rdd.partitions\", 4)` is needed only if your Cloudant service plan is *lite* because this plan has an access quota of 5 requests per second. \nSpark might run parallel jobs that might lead to more than 5 requests being made in one second. If this happens, a \"too many requests\" error is raised. To resolve this error, decrease the value for the *jsonstore.rdd.partitions* option to 2. For paid service plans this line can be commented out.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "cloudantdata=sqlContext.read.format(\"org.apache.bahir.cloudant\").\\\noption(\"cloudant.host\", host).\\\noption(\"cloudant.username\", username).\\\noption(\"cloudant.password\", password).\\\noption(\"view\",\"_design/iotp/_view/by-date\").\\\noption(\"jsonstore.rdd.partitions\", 4).\\\nload(dbName)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "An error occurred while calling o96.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 10 times, most recent failure: Lost task 0.9 in stage 0.0 (TID 18, yp-spark-lon02-env5-0109.bluemix.net, executor becf03b8-8fab-4d6d-ba7a-f35a6543fd8c): java.lang.RuntimeException: Stream '/files/spark-sql-cloudant_2.11-2.3.2.jar' was not found.\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:222)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\tat java.lang.Thread.run(Thread.java:812)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1631)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1620)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2004)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1103)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:69)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:328)\n\tat org.apache.bahir.cloudant.DefaultSource.create(DefaultSource.scala:124)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:95)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:812)\nCaused by: java.lang.RuntimeException: Stream '/files/spark-sql-cloudant_2.11-2.3.2.jar' was not found.\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:222)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\t... 1 more\n", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-6-78cb8eafb738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcloudantdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.bahir.cloudant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.host\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.username\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"view\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"_design/iotp/_view/by-date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jsonstore.rdd.partitions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o96.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 10 times, most recent failure: Lost task 0.9 in stage 0.0 (TID 18, yp-spark-lon02-env5-0109.bluemix.net, executor becf03b8-8fab-4d6d-ba7a-f35a6543fd8c): java.lang.RuntimeException: Stream '/files/spark-sql-cloudant_2.11-2.3.2.jar' was not found.\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:222)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\tat java.lang.Thread.run(Thread.java:812)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1631)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1620)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2004)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1103)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:69)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:328)\n\tat org.apache.bahir.cloudant.DefaultSource.create(DefaultSource.scala:124)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:95)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:812)\nCaused by: java.lang.RuntimeException: Stream '/files/spark-sql-cloudant_2.11-2.3.2.jar' was not found.\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:222)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\t... 1 more\n"
                    ], 
                    "ename": "Py4JJavaError"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "Observe the loaded data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "cloudantdata.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "All IoT data is located under the value column.\n\nNext, transform this hierarchical data frame into a flat one, and convert the timestamp from string type into a timestamp type.\nThe function withColumn adds a column named 'ts' to the data frame, and calculates it's content based on timestamp column (string), by using the to_ts function that was defined.\nThe cache() function of a data frame caches the data frame in memory, this is very useful when data is accessed repeatedly.\n\nThe *deviceData* is a temporary view in the Spark Session and can be used for select statements if you want to write raw SQL. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import udf, col, asc, desc,to_date, unix_timestamp, weekofyear, countDistinct\nfrom datetime import datetime\nfrom pyspark.sql.types import DateType, TimestampType, IntegerType", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# This function converts the string cell into a timestamp type:\nstr_to_ts =  udf (lambda d: datetime.strptime(d, \"%Y-%m-%dT%H:%M:%S.%fZ\"), TimestampType())\n\nsparkDf = cloudantdata.selectExpr(\"value.deviceId as deviceId\", \"value.deviceType as deviceType\", \"value.eventType as eventType\" ,  \"value.timestamp as timestamp\", \"value.data.*\",\"value.data.d.oa as oa\",\"value.data.d.ob as ob\",\"value.data.d.og as og\")\nsparkDf = sparkDf.withColumn('ts', col('timestamp'))\nsparkDf.cache()\nsparkDf.createOrReplaceTempView(\"deviceData\")\n\n# show the resulting schema and data \nsparkDf.printSchema()\nspark.sql(\"SELECT * from deviceData\").show(0)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Change the values of deviceId and deviceType for which you want to see visualizations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "deviceId = 'phone'\ndeviceType = 'simulator'", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Along with device movement data, you can also add acceleration data(ax,ay,az).\n\n## Data visualization and comprehension\n\n### Device Health \n\nIn this section we will see how to learn about the population of IoT devices and answer questions such as: \n1. How many reports each device type had?\n2. What is the breakdown of the devices per device type?\n3. How many reports have been sent by each device? \n4. How many reports each event type had? \n5. How many devices reported in a given time interval?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pprint\nfrom collections import Counter\nimport numpy as np\nfrom matplotlib import dates", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Use Spark to prepare the data for visualization, because Spark can support big data processing. When the data is ready for visualization, convert Spark data Frame into Pandas data Frame, because Pandas has good visualization support.\n\n#### How many reports each device type had? \n\nSetting the *deviceType* as index of the created Pandas data frame causes the bar plot to be aggregated by the deviceType. Call the plot function of the Pandas data frame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "EperDtDF = spark.sql(\"SELECT ts,deviceType from deviceData\").groupBy(\"deviceType\").count()\nEperDtDF.cache()\nEperDtDF.show()\n\nEperDtPanda = EperDtDF.toPandas().set_index('deviceType')\n\nax = EperDtPanda.plot(kind='bar',legend=False)\nax.set_xlabel(\"deviceType\")\nax.set_ylabel(\"events count\")\nax.set_title('count of events by deviceType')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### What is the breakdown of the devices per device type? \nThe bar chart is plotted in the same way as before, but now we will also show the pie chart of the data. Pandas data frame supports different plot types. Using pie generates a pie chart with percentage sizes of the pieces.\nTo write the actual count of the devices, instead of percentages, use the autopct argument - multiply by the total amount of devices and divide by 100 to get the actual numbers. \nThe total is calculated by using the *sum()* function of Pandas data frame, which sums up the device count of all the deviceTypes. The sum function returns a DataFrame, so use the *[0]* index to get only the value into the total.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "DperDtDF = spark.sql(\"SELECT deviceId,deviceType from deviceData\").groupBy(\"deviceType\").agg(countDistinct('deviceId'))\nEperDtDF.cache()\nDperDtDF.show()\n\n# bar chart of deviceId by deviceType\nEperDtPanda = DperDtDF.toPandas().set_index('deviceType')\n\nax = EperDtPanda.plot(kind='bar',legend=False)\nax.set_xlabel(\"deviceType\")\nax.set_ylabel(\"devices count\")\nax.set_title('count of deviceIds by deviceType')\n\n\n# Pie chart of deviceId by deviceType\nfig = plt.figure(figsize=(5,5))\nax = plt.subplot(111)\ntotal = EperDtPanda.sum()[0]\n\nax = EperDtPanda.plot(kind='pie', ax=ax, figsize=(5,5), legend=False, shadow=True, subplots=True, autopct=lambda p: '{:.0f}'.format(p * total / 100))\nplt.title(\"count of deviceIds by deviceType\")\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### How many reports have been sent by each device? ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "EperDdf = spark.sql(\"SELECT deviceId,ts from deviceData\").groupBy(\"deviceId\").count()####.sort()########\nEperDtDF.cache()\nEperDdf.show()\n\nEperDPanda = EperDdf.toPandas().set_index('deviceId')\n\nax = EperDPanda.plot(kind='bar',legend=False)\nax.set_xlabel(\"deviceId\")\nax.set_ylabel(\"events count\")\nax.set_title('count of events by deviceId')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### How many reports each event type had? ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "EperEtDF = spark.sql(\"SELECT eventType,ts from deviceData\").groupBy(\"eventType\").count()\nEperDtDF.cache()\nEperEtDF.show()\n\nEperEtPanda = EperEtDF.toPandas().set_index('eventType')\n\nax = EperEtPanda.plot(kind='bar',legend=False)\nax.set_xlabel(\"eventType\")\nax.set_ylabel(\"events count\")\nax.set_title('count of events by eventType')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### How many devices reported in a given time interval?\n\n**User input required**: Replace the year, month and day in the following cell to specify `start` and `end` interval.\n\nFor example:\n\n`start = datetime(2017, 7, 28, 0, 0, 0)\nend = datetime(2017, 7, 28, 23, 59, 59)`\n\nMake sure that the interval contains device events. You can narrow down the time interval as well. Then run the cell.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# set the time interval of interest\ndbDate = dbName.split(\"_\")[3]\ndbDate = dbDate.split(\"-\")\n\n# datetime(year, month, day, hours, minutes, seconds)\n#start = datetime(int(dbDate[0]), int(dbDate[1]), int(dbDate[2]), 0, 0, 0) \n#end = datetime(int(dbDate[0]), int(dbDate[1]), int(dbDate[2]), 23, 59, 59)\n\nstart = datetime(2017, 7, 28, 0, 0, 0)\nend = datetime(2017, 7, 28, 23, 59, 59)\n\nprint(start)\nprint(end)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "First we filter the data by a time interval, then group the resulting dataFrame by *deviceId*, and count the records for each *deviceId*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#filter by time interval\ndeviceMetaData = sparkDf.select('deviceId','deviceType','ts','timestamp','eventType').filter((col('ts')>=start) & (col('ts')<=end))\n\ndeviceMetaData.cache()\n#deviceMetaData.show()\n\n#how many devices reported in interval\nbyDevice = deviceMetaData.groupby(['deviceId']).count()\nbyDevice.cache()\n\nprint(\"Number of events by deviceId in the interval: \")\nbyDevice.show()\nprint(\"total number of devices reported in the interval:\", byDevice.count())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Count of rows by time span for a specific device, using the filter function of Spark DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "byDevice.filter(byDevice[\"deviceId\"]== deviceId).show() ##also show 5 with lowest counts", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Extract all the numeric columns for further analytics; only a subset of the numeric columns are selected for this demonstration:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#find all numeric columns of the DataFrame\nnumericCols = [name_dt for name_dt in sparkDf.dtypes if (('double' in name_dt[1]) or ('int' in name_dt[1]) or ('long' in name_dt[1]))] \n\n#numericCols is a list of pairs (columnName, dataType), here we select only the column name into the allkeys list\nallkeys = [x[0] for x in numericCols]\nprint(\"all numeric columns\", allkeys)\n\n#select only 5 numeric columns for further detailed visualizations\nkeys = ['oa','ob','og']\nprint(\"selected 3 numeric columns\", keys)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Device type sensor visualization \n\nSummary of sensor data that is reported by all devices of a device type, for example:\n\n1. What is the Average/Min/Max of all reported sensor values? \n2. Can I see a histogram of a sensor's output?   \n3. What is the correlation between two sensors?\n\n#### Average/Min/Max of all reported sensor values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.mllib.stat import Statistics\n\n#showing visualization for device type \n\n#show summary only for the selected 5 columns, for easier view, since we have too many columns to fit in a row\ndfKeysType = sparkDf.select(*keys).filter(sparkDf[\"deviceType\"]==deviceType)\ndfKeysType.cache()\n\ndfKeysType.describe().show()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Histogram of a device type sensor's output\n\n1.\tUse Spark DataFrame to prepare the histogram for each specific sensor (key) (using rdd.flatMap)\n2.\tCreate Pandas DataFrame from the calculated histogram with 2 columns: \"bin\" and \"frequency\".\n3.\tPlot the histogram using Pandas plot function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "for key in keys:\n    histogram = dfKeysType.select(key).rdd.flatMap(lambda x: x).histogram(11)\n    \n    #print histogram\n    pandaDf = pd.DataFrame(list(zip(list(histogram)[0],list(histogram)[1])),columns=['bin','frequency']).set_index('bin')\n    ax =pandaDf.plot(kind='bar')\n    ax.set_ylabel(\"frequency\")\n    ax.set_title('Histogram of ' + key + ' sensor output')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Correlation between two sensors\n\nCorrelation between two sensors can be plotted using Pandas plot with kind='scatter'. Remember that *dfKeysType* is a Spark DataFrame that includes only our selected 5 columns and is filtered by *deviceType*. You can also filter by *timestamp* to decrease the amount of data that is output.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "key1=\"oa\"\nkey2=\"ob\"\n\npandaDF = dfKeysType.toPandas()\nax = pandaDF.plot(kind='scatter', x=key1, y=key2, s=5, figsize=(7,7))\nax.set_title('Relationship between ' + key1 + ' and ' + key2 )", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "To view all the correlations of the selected 5 columns, together with a histogram on a diagonal, use the Pandas scatter_matrix function:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pd.plotting.scatter_matrix(pandaDF, figsize=(18,12))\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A correlation matrix can be plotted, using Pandas corr() function on the DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "correlations = pandaDF.corr()\n\n# plot correlation matrix\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,5,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(keys)\nax.set_yticklabels(keys)\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Sensor deep dive\n\nSensor deep dive output is similar to the device type sensor visualization but the data is filtered by *deviceId*.\n\n#### Average/Min/Max of all reported sensor values by the device", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#showing visualization for specific deviceID set above\n#show summary only for a selected group of columns, for easier view, since we have too many columns to fit in a row\ndfKeysDev = sparkDf.select(*keys).filter(sparkDf[\"deviceId\"]==deviceId)\ndfKeysDev.cache()\n\ndfKeysDev.describe().show()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to show the range of the data. Beyond the whiskers, data are considered outliers and are plotted as individual points.\n\nA box plot for each devices sensor, produced with the Pandas plot function with kind=\"box\":", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pandaDF = dfKeysDev.toPandas()\npandaDF.plot(kind='box', subplots=True, layout=(10,3), sharex=False, sharey=False, figsize=(25,60))\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Histogram of a device's sensor output", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "for key in keys:\n    try:\n        #The histogram is built with spark. Only the groupped by bins data will be converted to Pandas DataFrame\n        histogram = dfKeysDev.select(key).rdd.flatMap(lambda x: x).histogram(11)\n\n        #print histogram\n        pandaDf = pd.DataFrame(zip(list(histogram)[0],list(histogram)[1]),columns=['bin','frequency']).set_index('bin')\n        ax = pandaDf.plot(kind='bar')\n        ax.set_ylabel(\"frequency\")\n        ax.set_title('Histogram of ' + key + ' sensor output')\n   \n    except: \n        print(\"no values for sensor \" + key + \" for device \" + deviceId + \"\\n\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The histograms can also be built more easily with Pandas DataFrame, in case the dfKeysDev DataFrame is not too large. For the case of big data, spark is more scalable.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pandaDF = dfKeysDev.toPandas()\npandaDF.hist(layout=(3,3), sharex=False, figsize=(20,15))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Density Plots\n\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n\n**Note**: here we convert the data into Pandas DataFrame, after we filtered by deviceId and selected a subset of keys. In case this is still too much data for the Pandas DataFrame to handle, consider selecting fewer keys and filtering by time interval.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pandaDF = dfKeysDev.toPandas()\n\nax = pandaDF.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(20,15))\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### How a specific device sensor value changes over time\n\nMaximum, minimum, and average lines are shown on plots.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import mean, min, max\n\n#showing visualization for specific deviceID set above\n\nprint(keys)\nfor key in keys:\n    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n    df.cache()\n    if (df.count() > 0):\n        pandaDF = df.toPandas()\n        \n        ax = pandaDF.plot(x='ts', y=key , legend=False, figsize=(15,9), ls='-', marker='o')\n        ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n        ax.set_title(key + ' over time')\n        ax.set_ylabel(key)\n        ax.grid(True)\n        \n        # Draw lines to showcase the upper and lower threshold\n        ax.axhline(y=pandaDF[key].min(),c=\"red\",linewidth=2,zorder=0)\n        ax.axhline(y=pandaDF[key].max(),c=\"red\",linewidth=2,zorder=0)\n        ax.axhline(y=pandaDF[key].mean(),c=\"green\",linewidth=2,zorder=0, ls='--')\n    \n        ax.autoscale_view()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Note**: Data can be aggregated by intervals, for example by seconds/minutes/hours in Spark and displayed as aggregated plots (average, minimum, maximum).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import window\nfrom functools import reduce\n\n#showing visualization for specific deviceID set above\n\nfor key in keys:\n    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n\n    df = df.groupBy(\"deviceId\", window(\"ts\", \"30 seconds\")).agg(max(key), min(key), mean(key))\n    #df = df.groupBy(\"deviceId\", window(\"ts\", \"1 minute\")).agg(max(key), min(key), mean(key))\n    #df.groupBy(\"deviceId\", window(\"ts\", \"5 minutes\")).agg(max(key), min(key), mean(key))\n    #df.groupBy(\"deviceId\", window(\"ts\", \"1 hour\")).agg(max(key), min(key), mean(key))\n    \n    #change automatic name of aggregated column\n    oldColumns = df.schema.names\n    newColumns = [\"deviceId\", \"window\", \"max\", \"min\", \"avg\"]\n    df = reduce(lambda df, idx: df.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df)\n    \n    win_to_ts =  udf (lambda d: d.start, TimestampType())\n\n    df = df.withColumn('ts', win_to_ts(col('window')))\n    df = df.select('deviceId','ts',\"max\", \"min\", \"avg\")\n    df.cache()\n    \n    if (df.count() > 0):\n        pandaDF = df.toPandas()\n\n        ax = pandaDF.plot(x='ts', y='min', legend=True, figsize=(15,9), ls='-', marker='o', c=\"red\")\n        ax = pandaDF.plot(ax=ax, x='ts', y='max', legend=True, figsize=(15,9), ls='-', marker='o', c=\"red\")\n        ax = pandaDF.plot(ax=ax, x='ts', y='avg', legend=True, figsize=(15,9), ls='-', marker='o', c=\"green\")\n        \n        #'S' secondly frequency, 'T' minutely frequency, 'H' hourly frequency\n        xtick = pd.date_range(start=pandaDF['ts'].min(), end=pandaDF['ts'].max(), freq='30S')\n        ax.set_xticks(xtick)\n\n        ax.xaxis_date()\n        ax.set_title(key + ' over time groupped by 30 sec')\n        ax.set_ylabel(key)\n    \n        ax.autoscale_view()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "For Big Data is also possible to show the plot around the extremum point. In the example below we show values spanned in 4 seconds around the maximum value. Note that the aggregation is done inside Spark and Pandas is used for visualization.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import mean, min, max\n\n#showing visualization for specific deviceID\n\nfor key in keys:\n    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n    df.cache()\n    if (df.count() > 0):\n        #find max and filter around them\n        max_value = df.select(max(key)).collect()[0][0]\n        maxts = df.filter(df[key] == max_value).rdd.map(lambda r: r['ts']).collect()[0]\n        startts = maxts - pd.DateOffset(seconds=4) #(minutes=2)#(days=15)\n        endts = maxts + pd.DateOffset(seconds=4)\n        df_max = df.filter((col('ts')>=startts) & (col('ts')<=endts))\n\n        pandaDF = df_max.toPandas()\n\n        ax = pandaDF.plot(x='ts', y=key , legend=False, figsize=(15,9), ls='-', marker='o')\n        ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n        ax.set_title(key + ' over time')\n        ax.set_ylabel(key)\n        ax.grid(True)\n        \n        # Draw lines to showcase the upper and lower threshold\n        ax.axhline(y=pandaDF[key].min(),c=\"red\",linewidth=2,zorder=0)\n        ax.axhline(y=pandaDF[key].max(),c=\"red\",linewidth=2,zorder=0)\n        ax.axhline(y=pandaDF[key].mean(),c=\"green\",linewidth=2,zorder=0, ls='--')\n    \n        ax.autoscale_view()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Compare between the sensor values of devices over time\n\nThe *dfKeysDev* DataFrame contains only keys columns, with no ts column, so we will create a new data frame that will also include the *ts*:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#showing visualization for specific deviceID\n\ncolumns = list(keys)\ncolumns.append('ts')\ndf = sparkDf.select(*columns).filter(sparkDf[\"deviceId\"]==deviceId)\n\npandaDF = df.toPandas().set_index('ts')\nax = pandaDF.plot(figsize=(15,9),ls='', marker='o')   \nax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\nax.set_title(', '.join(keys) + ' over time')\nax.grid(True)\nax.autoscale_view()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pandaDF = dfKeysDev.toPandas()\n\npd.plotting.scatter_matrix(pandaDF, figsize=(18,12))\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Anomaly detection\n\n\nAnomaly detection will be performed using *z-score*. *z-score* is a standard score that indicates how many standard deviations an element is from the mean. A z-score can be calculated from the following formula:\n`z = (X - \u00b5) / \u03c3`\nwhere z is the *z-score*, *X* is the value of the element, *\u00b5* is the population mean, and *\u03c3* is the standard deviation.\n\nA higher *z-score* value represents a larger deviation from the mean value which can be interpreted as abnormal.\n\nWe will calculate *z-score* for each selected column (sensor) of each device type, and plot only the sensors that have spikes. We define a spike in the following function spike(row), by reported value having z-score above 3 or below -3. Observe that the values for which the *z-score* is above 3 or below -3, are marked as abnormal events in the following graph.\n\n**Note**: The code triggers a number of Spark jobs and might take a few seconds to finish.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# ignore warnings if any\nimport warnings\nfrom pyspark.sql.functions import mean, min, max, mean, stddev\n\nwarnings.filterwarnings('ignore')\n\n'''\nThis function detects the spike and dip by returning a non-zero value \nwhen the z-score is above 3 (spike) and below -3(dip). Incase if you \nwant to capture the smaller spikes and dips, lower the zscore value from \n3 to 2 in this function.\n'''\nupperThreshold = 3\nlowerThreshold = -3\ndef spike(row):\n    if(row['zscore'] >=upperThreshold or row['zscore'] <=lowerThreshold):\n        return row[key]\n    else:\n        return 0\n\n#get the list of available devices\ndeviceTypes = sparkDf.select(\"deviceType\").groupBy(\"deviceType\").count().rdd.map(lambda r: r[0]).collect()\n\n#calculate for each device type and each key\nfor devt in deviceTypes:\n    for key in keys:\n        df = spark.sql(\"SELECT deviceType, ts,\" + key +\" from deviceData where deviceType='\" + devt + \"'\").where(col(key).isNotNull())\n        if (df.count() > 0):\n            pandaDF = df.toPandas().set_index(\"ts\")\n            \n            # calculate z-score and populate a new column\n            pandaDF['zscore'] = (pandaDF[key] - pandaDF[key].mean())/pandaDF[key].std(ddof=0)\n\n            #add new column - spike, and calculate its value based on the thresholds, usinf spike function, defined above\n            pandaDF['spike'] = pandaDF.apply(spike, axis=1)\n            \n            \n            #plot the chart, only if spikes were detected (not all values of \"spike\" are zero)\n            if (pandaDF['spike'].nunique() > 1):\n                # select rows that are required for plotting\n                plotDF = pandaDF[[key,'spike']]\n                #calculate the y minimum value\n                y_min = (pandaDF[key].max() - pandaDF[key].min()) / 10\n                fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n                ax.set_ylim(plotDF[key].min() - y_min, plotDF[key].max() + y_min)\n                x_filt = plotDF.index[plotDF.spike != 0]\n                plotDF['spikes'] = plotDF[key]\n                y_filt = plotDF.spikes[plotDF.spike != 0]\n                #Plot the raw data in blue colour\n                line1 = ax.plot(plotDF.index, plotDF[key], '-', color='blue', animated = True, linewidth=1, marker='o')\n                #plot the anomalies in red circle\n                line2 = ax.plot(x_filt, y_filt, 'ro', color='red', linewidth=2, animated = True)\n                #Fill the raw area\n                ax.fill_between(plotDF.index, (pandaDF[key].min() - y_min), plotDF[key], interpolate=True, color='blue',alpha=0.6)\n\n                # calculate the sensor value that is corresponding to z-score that defines a spike\n                valUpperThreshold = (pandaDF[key].std(ddof=0) * upperThreshold) + pandaDF[key].mean()\n                # calculate the sensor value that is corresponding to z-score that defines a dip\n                valLowerThreshold = (pandaDF[key].std(ddof=0) * lowerThreshold) + pandaDF[key].mean()\n\n                #plot the thresholds\n                ax.axhline(y=valUpperThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dashed',label='Upper threshold')\n                ax.axhline(y=valLowerThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dotted',label='Lower threshold')\n                \n                # Label the axis\n                ax.set_xlabel(\"Sequence\",fontsize=20)\n                ax.set_ylabel(key,fontsize=20)\n                ax.set_title(\"deviceType: \" + devt + \" sensor:\" + key)\n                plt.tight_layout()\n                plt.legend()\n                \n                print(\"Device Type: \" + devt + \", sensor: \" + key)\n                print(\"Upper treshould based on the z-score calculation: \" , upperThreshold , \": \" , valUpperThreshold)\n                print(\"Lower treshould based on the z-score calculation: \", lowerThreshold, \": \" , valLowerThreshold)\n                \n                plt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The red marks indicate unexpected spikes where the z-score value is greater than 3 or less than -3. To detect lower spikes, modify the value to 2 or less. Similarly, if you want to detect only the higher spikes, increase the z-score value from 3 to 4 or more.\n\nFor complete solution tutorial, refer [Gather, Visualize and Analyze IoT data](http://console.bluemix.net/docs/tutorials/gather-visualize-analyze-iot-data.html#gather-visualize-and-analyze-iot-data)", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}